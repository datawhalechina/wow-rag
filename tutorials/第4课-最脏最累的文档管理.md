上节课的txt文件的制作需要考虑分块。分块的质量直接决定着回答质量。因为如果原本应该属于一个文本块的内容如果被分到不同文本块，那么就会有回答不全的问题。

### 文字版PDF的分块

由于很多PDF文件制作得比较精致，里面标题都有显著的特征。通常检测PDF中的字体和字号就能准确区分哪些文字是字体。pymupdf这个库给我们提供了比较强大的解析功能。

```python
import pymupdf  # 导入PyMuPDF

file_path = "build_arm_part1.pdf"

def analyze_text_format(pdf_path):
    """
    分析PDF指定页面中文本的格式，包括字体、字号和是否粗体。
    
    参数:
        pdf_path: PDF文件路径
        page_num: 要分析的页面编号（从0开始）
    """
    text_list = []
    doc = pymupdf.open(pdf_path)
    for i,page in enumerate(doc):
        # 获取页面文本的完整字典结构
        page_dict = page.get_text("dict")
        # 遍历blocks -> lines -> spans
        for block in page_dict["blocks"]:
            block_text = []
            score = 0
            if block["bbox"][1]<740 and block["bbox"][3]>50:
                if "lines" in block:  # 确保是文本块
                    if len(block["lines"])==1:
                        score += 3
                    if block["lines"][0]["spans"][0]["font"] == "Arial-BoldMT":
                        score += 1
                    if block["lines"][0]["spans"][0]["flags"] == 16:
                        score += 1
                    if int(block["lines"][0]["spans"][0]["size"]) >= 14:
                        score += 1
                    for line in block["lines"]:
                        for span in line["spans"]:
                            # 提取关键信息
                            text = span["text"]
                            font = span["font"]  # 字体名称
                            size = span["size"]  # 字号（浮点数，单位是pt）
                            flags = span["flags"]  # 样式标志位（二进制位存储）
                            block_text.append(text)
                
                    block_dict = {"page_num":i ,"score":score, "block_text":" ".join(block_text)}
                    text_list.append(block_dict)
    doc.close()
    return text_list

# 把PDF的文字都抓出来，每个短语都有page_num、score和block_text三个属性。
list_result = analyze_text_format(file_path)

# 把抓出来的短句进行分块。
# score大于4分的我们认定为标题
chunk_list = []
tmp = []
# 每个分块的字符数最多3000字符，大约合500英文词，一页PDF内容。
# 对于跨页的内容，如果应该是一段的，我们需要合并为一段。
chunk_size = 3000
for i, para in enumerate(list_result):
    if len(para["block_text"]) > chunk_size:
        chunks = [para["block_text"][i:i+chunk_size] for i in range(0, len(para["block_text"]), chunk_size)]
        chunk_list.extend(chunks)
    elif len(" ".join(tmp) + para["block_text"]) > chunk_size:
        tmp_title = tmp[0]
        chunk_list.append(tmp)
        tmp = [tmp_title]
    if para["score"]>4:
        if len(tmp)>0:
            chunk_list.append(tmp)
        tmp = [para["block_text"]]
    elif list_result[i-1]["block_text"][-1] not in ".:;" and list_result[i]["block_text"][0].islower():
        tmp[-1] += " "+para["block_text"]
    else:
        tmp.append(para["block_text"])

# 把分块后的内容写入txt文件，每个分块以'\n\n'分隔。
with open("output_chunk.txt", 'w', encoding='utf-8') as f:
    for chunk in chunk_list:
        chunk_text = "\n".join(chunk)
        f.write(chunk_text + '\n\n')
```



文档管理意味着对保存在硬盘上的index进行增删改查。我们对其中一个index进行分析

查看index下面的所有切片
```python
print(index.docstore.docs)
```

你的输出大概会是这样的内容
```plaintext
{'27fe7851-5e9e-4b76-864c-d202e602a62c': TextNode(id_='27fe7851-5e9e-4b76-864c-d202e602a62c', embedding=None, metadata={'file_path': 'docs\\book1.txt', 'file_name': 'book1.txt', 'file_type': 'text/plain', 'file_size': 448588, 'creation_date': '2025-07-13', 'last_modified_date': '2024-05-27'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='987f4216-3aa0-46d0-9264-65ae9460ca84', node_type='4', metadata={'file_path': 'docs\\book1.txt', 'file_name': 'book1.txt', 'file_type': 'text/plain', 'file_size': 448588, 'creation_date': '2025-07-13', 'last_modified_date': '2024-05-27'}, hash='4f844eb1f5f01a73f5c3aa0b98ac13d69162945a4d35c5031214474892beffa8'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='7e298ff5-02c1-423b-8832-1a25392a33c9', node_type='1', metadata={}, hash='516e3ce5a8d2b9c39acc1e7f7046996eb615399618e82a9e1b652ea6672dac92')}, metadata_template='{key}: {value}', metadata_separator='\n', text='the boy who lived mr. and mrs. 后面省略大堆内容……', mimetype='text/plain', start_char_idx=0, end_char_idx=4036, metadata_seperator='\n', text_template='{metadata_str}\n\n{content}'),……
```

在加载文档时，llama-index会对其进行切片，每段切片大概有几千字的文本内容，除此以外还包含一些元数据，比如文件地址、文件名、文件类型、大小、处理日期等。在llama-index的RAG流程中，切片的元数据和文本内容将会被交付给LLM作为知识库的一部分。具体传入LLM的交付格式则是末尾的`text_template='{metadata_str}\n\n{content}'`。


查看index下面的所有node的id
```python
print(index.index_struct.nodes_dict)
```
运行后会看到这样的内容，这些是llama-index对处理出的切片编的id。
```plaintext
{'27fe7851-5e9e-4b76-864c-d202e602a62c': '27fe7851-5e9e-4b76-864c-d202e602a62c',
 '7e298ff5-02c1-423b-8832-1a25392a33c9': '7e298ff5-02c1-423b-8832-1a25392a33c9',
 '12a64a14-19ff-416a-87d7-c85aa03bb9ab': '12a64a14-19ff-416a-87d7-c85aa03bb9ab',
 '2fbe817b-196e-4546-9b01-1e8eda3229c3': '2fbe817b-196e-4546-9b01-1e8eda3229c3',
 'ab8b7165-d290-4459-8bfc-7bd21f503f49': 'ab8b7165-d290-4459-8bfc-7bd21f503f49',
……
```

查看index下面所有有ref的文档的信息
```python
print(index.ref_doc_info)
```
下面的`987f4216-3aa0-46d0-9264-65ae9460ca84`是被切片的文件的id，它的value则是其对应各个切片的id。
```
{'987f4216-3aa0-46d0-9264-65ae9460ca84': RefDocInfo(node_ids=['27fe7851-5e9e-4b76-864c-d202e602a62c', '7e298ff5-02c1-423b-8832-1a25392a33c9', '12a64a14-19ff-416a-87d7-c85aa03bb9ab', '2fbe817b-196e-4546-9b01-1e8eda3229c3', 'ab8b7165-d290-4459-8bfc-7bd21f503f49', 'b1896fee-c9fb-4041-9273-49e98f172551', 'bdcb811f-aa8c-4f54-bc0a-a0775e81a4e9', ……], metadata={'file_path': 'docs\\book1.txt', 'file_name': 'book1.txt', 'file_type': 'text/plain', 'file_size': 448588, 'creation_date': '2025-07-13', 'last_modified_date': '2024-05-27'})}
```


查看任意给定id的node详细信息
```python
index.docstore.get_node('7e298ff5-02c1-423b-8832-1a25392a33c9')
# 或者 index.docstore.docs['7e298ff5-02c1-423b-8832-1a25392a33c9']
```

下面是结果
```
TextNode(id_='7e298ff5-02c1-423b-8832-1a25392a33c9', embedding=None, metadata={'file_path': 'docs\\book1.txt', 'file_name': 'book1.txt', 'file_type': 'text/plain', 'file_size': 448588, 'creation_date': '2025-07-13', 'last_modified_date': '2024-05-27'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='987f4216-3aa0-46d0-9264-65ae9460ca84', node_type='4', metadata={'file_path': 'docs\\book1.txt', 'file_name': 'book1.txt', 'file_type': 'text/plain', 'file_size': 448588, 'creation_date': '2025-07-13', 'last_modified_date': '2024-05-27'}, hash='4f844eb1f5f01a73f5c3aa0b98ac13d69162945a4d35c5031214474892beffa8'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='27fe7851-5e9e-4b76-864c-d202e602a62c', node_type='1', metadata={'file_path': 'docs\\book1.txt', 'file_name': 'book1.txt', 'file_type': 'text/plain', 'file_size': 448588, 'creation_date': '2025-07-13', 'last_modified_date': '2024-05-27'}, hash='47ab8fe8b24e898419c11d0cad4942f8a1b941ff7486925ad6491682599d68d0'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='12a64a14-19ff-416a-87d7-c85aa03bb9ab', node_type='1', metadata={}, hash='956f5577e629e228a8b2e741549b5d4388c0114f933fddf48142a64dd7b9949b')}, metadata_template='{key}: {value}', metadata_separator='\n', text='people in cloaks. mr. dursley couldn’t bear people who dressed in funny clothes — the getups you saw on young people! he supposed this was some stupid new fashion. he drummed his fingers on the steering wheel and his eyes fell on a 省略很多字……', mimetype='text/plain', start_char_idx=3271, end_char_idx=7425, metadata_seperator='\n', text_template='{metadata_str}\n\n{content}')
```


删除一个节点，非必要尽量不要尝试
```python
# index.docstore.delete_document('7e298ff5-02c1-423b-8832-1a25392a33c9')
```


新增节点
```python
index.insert_nodes([doc_single])
```
注意这里的doc_single必须是一个 TextNode 对象。例如上文查看node时输出的那个。
TextNode 对象也可以自己构造。构造方式为：

```python
from llama_index.core.schema import TextNode
nodes = [
    TextNode(
        text="The Shawshank Redemption",
        metadata={
            "author": "Stephen King",
            "theme": "Friendship",
            "year": 1994,
        },
    ),
    TextNode(
        text="The Godfather",
        metadata={
            "director": "Francis Ford Coppola",
            "theme": "Mafia",
            "year": 1972,
        },
    )
]
index.insert_nodes(nodes)
```

或者仿照前一节课的从文档构造节点的方式。
```python
# 从指定文件读取，输入为List
from llama_index.core import SimpleDirectoryReader,Document
documents = SimpleDirectoryReader(input_files=['./docs/另一个手册.txt']).load_data()
from llama_index.core.node_parser import SentenceSplitter
transformations = [SentenceSplitter(chunk_size = 512)]

from llama_index.core.ingestion.pipeline import run_transformations
nodes = run_transformations(documents, transformations=transformations)
index.insert_nodes(nodes)
```

至于修改，不建议手动修改已有内容，建议删了再改。


