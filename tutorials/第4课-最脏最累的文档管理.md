上节课的txt文件的制作需要考虑分块。分块的质量直接决定着回答质量。因为如果原本应该属于一个文本块的内容如果被分到不同文本块，那么就会有回答不全的问题。

### 文字版PDF的分块

由于很多PDF文件制作得比较精致，里面标题都有显著的特征。通常检测PDF中的字体和字号就能准确区分哪些文字是字体。pymupdf这个库给我们提供了比较强大的解析功能。

```python
import pymupdf  # 导入PyMuPDF

file_path = "build_arm_part1.pdf"

def analyze_text_format(pdf_path):
    """
    分析PDF指定页面中文本的格式，包括字体、字号和是否粗体。
    
    参数:
        pdf_path: PDF文件路径
        page_num: 要分析的页面编号（从0开始）
    """
    text_list = []
    doc = pymupdf.open(pdf_path)
    for i,page in enumerate(doc):
        # 获取页面文本的完整字典结构
        page_dict = page.get_text("dict")
        # 遍历blocks -> lines -> spans
        for block in page_dict["blocks"]:
            block_text = []
            score = 0
            if block["bbox"][1]<740 and block["bbox"][3]>50:
                if "lines" in block:  # 确保是文本块
                    if len(block["lines"])==1:
                        score += 3
                    if block["lines"][0]["spans"][0]["font"] == "Arial-BoldMT":
                        score += 1
                    if block["lines"][0]["spans"][0]["flags"] == 16:
                        score += 1
                    if int(block["lines"][0]["spans"][0]["size"]) >= 14:
                        score += 1
                    for line in block["lines"]:
                        for span in line["spans"]:
                            # 提取关键信息
                            text = span["text"]
                            font = span["font"]  # 字体名称
                            size = span["size"]  # 字号（浮点数，单位是pt）
                            flags = span["flags"]  # 样式标志位（二进制位存储）
                            block_text.append(text)
                
                    block_dict = {"page_num":i ,"score":score, "block_text":" ".join(block_text)}
                    text_list.append(block_dict)
    doc.close()
    return text_list

# 把PDF的文字都抓出来，每个短语都有page_num、score和block_text三个属性。
list_result = analyze_text_format(file_path)

# 把抓出来的短句进行分块。
# score大于4分的我们认定为标题
chunk_list = []
tmp = []
# 每个分块的字符数最多3000字符，大约合500英文词，一页PDF内容。
# 对于跨页的内容，如果应该是一段的，我们需要合并为一段。
chunk_size = 3000
for i, para in enumerate(list_result):
    if len(para["block_text"]) > chunk_size:
        chunks = [para["block_text"][i:i+chunk_size] for i in range(0, len(para["block_text"]), chunk_size)]
        chunk_list.extend(chunks)
    elif len(" ".join(tmp) + para["block_text"]) > chunk_size:
        tmp_title = tmp[0]
        chunk_list.append(tmp)
        tmp = [tmp_title]
    if para["score"]>4:
        if len(tmp)>0:
            chunk_list.append(tmp)
        tmp = [para["block_text"]]
    elif list_result[i-1]["block_text"][-1] not in ".:;" and list_result[i]["block_text"][0].islower():
        tmp[-1] += " "+para["block_text"]
    else:
        tmp.append(para["block_text"])

# 把分块后的内容写入txt文件，每个分块以'\n\n'分隔。
with open("output_chunk.txt", 'w', encoding='utf-8') as f:
    for chunk in chunk_list:
        chunk_text = "\n".join(chunk)
        f.write(chunk_text + '\n\n')
```



文档管理意味着对保存在硬盘上的index进行增删改查。我们先来看看针对faiss向量存储的管理方式。

查看index下面的所有文档
```python
print(index.docstore.docs)
```

查看index下面的所有node的id
```python
print(index.index_struct.nodes_dict)
```

查看index下面所有有ref的文档的信息
```python
print(index.ref_doc_info)
```

查看任意给定id的node详细信息
```python
index.docstore.get_node('51595901-ebe3-48b5-b57b-dc8794ef4556')
# 或者 index.docstore.docs['51595901-ebe3-48b5-b57b-dc8794ef4556']
```


删除一个节点，删除这个操作尽量不要尝试，可能会导致后面的代码运行出错。
```python
# index.docstore.delete_document('51595901-ebe3-48b5-b57b-dc8794ef4556')
```


新增节点
```python
index.insert_nodes([doc_single])
```
注意这里的doc_single必须是一个 TextNode 对象。例如上文查看node时输出的那个。
TextNode 对象也可以自己构造。构造方式为：

```python
from llama_index.core.schema import TextNode
nodes = [
    TextNode(
        text="The Shawshank Redemption",
        metadata={
            "author": "Stephen King",
            "theme": "Friendship",
            "year": 1994,
        },
    ),
    TextNode(
        text="The Godfather",
        metadata={
            "director": "Francis Ford Coppola",
            "theme": "Mafia",
            "year": 1972,
        },
    )
]
index.insert_nodes(nodes)
```

或者仿照前一节课的从文档构造节点的方式。
```python
# 从指定文件读取，输入为List
from llama_index.core import SimpleDirectoryReader,Document
documents = SimpleDirectoryReader(input_files=['./docs/另一个手册.txt']).load_data()
from llama_index.core.node_parser import SentenceSplitter
transformations = [SentenceSplitter(chunk_size = 512)]

from llama_index.core.ingestion.pipeline import run_transformations
nodes = run_transformations(documents, transformations=transformations)
index.insert_nodes(nodes)
```
至于改的话，没有很方便的方式。先删除再新增吧。


