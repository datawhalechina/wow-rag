{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们计划采用Llama-index来做RAG。Langchain也是个不错的选择。我们之后会加入Langchain的版本，不过目前只做了Llama-index的教程。干活之前我们先准备好必备模型。一个llm模型，和一个embedding模型。\n",
    "\n",
    "想要借助Llama-index构建llm和embedding模型，我们大体上有四种思路。\n",
    "\n",
    "- 第一个思路：使用Llama-index为各个厂家单独构建的服务，比如Llama-index为智谱和零一万物构建了专门的包，我们可以直接安装使用。\n",
    "- 第二个思路：如果Llama-index没有为某个厂家构建的服务，我们可以借助Llama-index为openai构建的库。只要我们国内的模型是openai兼容型的，我们就可以稍微修改一下源码直接使用。\n",
    "- 第三个思路：我们可以利用Llama-index提供的自定义类来自定义模型。\n",
    "- 第四个思路：我们可以在本地安装Ollama，在本地安装好模型，然后在Llama-index中使用Ollama的服务。\n",
    "\n",
    "\n",
    "\n",
    "# 第一个思路，使用Llama-index为智谱构建的专门的包，直接安装最新版本即可。\n",
    "这种方法对智谱来说最友好，但是对于国内的大模型，Llama-index只为智谱和零一万物构建了专用包，其他厂家的大模型，我们就没办法这样使用了。顺便提一句，零一万物现在对学生的优惠有36元，可以去官网免费试用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-core\n",
    "%pip install llama-index-embeddings-zhipuai\n",
    "%pip install llama-index-llms-zhipuai\n",
    "%pip install llama-index-readers-file\n",
    "%pip install llama-index-vector-stores-faiss\n",
    "%pip install llamaindex-py-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跟第一节课的开头一样，咱们现在先把四样前菜准备一下吧：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "# 从环境变量中读取api_key\n",
    "api_key = os.getenv('ZHIPU_API_KEY')\n",
    "base_url = \"https://open.bigmodel.cn/api/paas/v4/\"\n",
    "chat_model = \"glm-4-flash\"\n",
    "emb_model = \"embedding-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 配置对话模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.zhipuai import ZhipuAI\n",
    "llm = ZhipuAI(\n",
    "    api_key = api_key,\n",
    "    model = chat_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试对话模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。\n"
     ]
    }
   ],
   "source": [
    "# 测试对话模型\n",
    "response = llm.complete(\"你是谁？\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "我是一个\n",
      "\n",
      "我是一个人工智能\n",
      "\n",
      "我是一个人工智能助手\n",
      "\n",
      "我是一个人工智能助手，\n",
      "\n",
      "我是一个人工智能助手，专门\n",
      "\n",
      "我是一个人工智能助手，专门被\n",
      "\n",
      "我是一个人工智能助手，专门被设计\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，完全是\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，完全是基于\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，完全是基于软件\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，完全是基于软件运行的\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，完全是基于软件运行的。\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，完全是基于软件运行的。我的\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，完全是基于软件运行的。我的目的是\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，完全是基于软件运行的。我的目的是辅助\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，完全是基于软件运行的。我的目的是辅助用户\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，完全是基于软件运行的。我的目的是辅助用户获取\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，完全是基于软件运行的。我的目的是辅助用户获取所需\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，完全是基于软件运行的。我的目的是辅助用户获取所需的信息\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，完全是基于软件运行的。我的目的是辅助用户获取所需的信息和\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，完全是基于软件运行的。我的目的是辅助用户获取所需的信息和帮助\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，完全是基于软件运行的。我的目的是辅助用户获取所需的信息和帮助。\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，完全是基于软件运行的。我的目的是辅助用户获取所需的信息和帮助。您\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，完全是基于软件运行的。我的目的是辅助用户获取所需的信息和帮助。您有什么\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，完全是基于软件运行的。我的目的是辅助用户获取所需的信息和帮助。您有什么问题\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，完全是基于软件运行的。我的目的是辅助用户获取所需的信息和帮助。您有什么问题我可以\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，完全是基于软件运行的。我的目的是辅助用户获取所需的信息和帮助。您有什么问题我可以帮\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，完全是基于软件运行的。我的目的是辅助用户获取所需的信息和帮助。您有什么问题我可以帮您\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，完全是基于软件运行的。我的目的是辅助用户获取所需的信息和帮助。您有什么问题我可以帮您解答\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，完全是基于软件运行的。我的目的是辅助用户获取所需的信息和帮助。您有什么问题我可以帮您解答的吗\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，完全是基于软件运行的。我的目的是辅助用户获取所需的信息和帮助。您有什么问题我可以帮您解答的吗？\n",
      "\n",
      "我是一个人工智能助手，专门被设计用来帮助回答问题、提供建议和信息。我没有实体，完全是基于软件运行的。我的目的是辅助用户获取所需的信息和帮助。您有什么问题我可以帮您解答的吗？"
     ]
    }
   ],
   "source": [
    "# 测试对话模型流式输出\n",
    "response = llm.stream_complete(\"你是谁?\")\n",
    "for chunk in response:\n",
    "    print('\\n')\n",
    "    print(chunk, end=\"\",flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 配置嵌入模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置嵌入模型\n",
    "from llama_index.embeddings.zhipuai import ZhipuAIEmbedding\n",
    "embedding = ZhipuAIEmbedding(\n",
    "    api_key = api_key,\n",
    "    model = emb_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试嵌入模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, list)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试嵌入模型\n",
    "emb = embedding.get_text_embedding(\"你好呀呀\")\n",
    "len(emb), type(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出 (1024, list)\n",
    "\n",
    "说明配置成功。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二个思路，借助openai的库，里面接入国内的模型\n",
    "直接安装最新版本\n",
    "\n",
    "\n",
    "如果是在jupyter notebook里运行，需要先用魔法命令安装这些库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://repo.huaweicloud.com/repository/pypi/simple\n",
      "Requirement already satisfied: llama-index-core in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (0.12.19)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (2.0.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core) (3.11.12)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core) (2025.3.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core) (2.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core) (2.10.6)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.18.3)\n",
      "Requirement already satisfied: click in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from nltk>3.8.1->llama-index-core) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from nltk>3.8.1->llama-index-core) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from nltk>3.8.1->llama-index-core) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from pydantic>=2.8.0->llama-index-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from pydantic>=2.8.0->llama-index-core) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.31.0->llama-index-core) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.31.0->llama-index-core) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.31.0->llama-index-core) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.31.0->llama-index-core) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from dataclasses-json->llama-index-core) (3.26.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from httpx->llama-index-core) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from httpx->llama-index-core) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core) (24.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from anyio->httpx->llama-index-core) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from anyio->httpx->llama-index-core) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://repo.huaweicloud.com/repository/pypi/simple\n",
      "Requirement already satisfied: llama-index-embeddings-openai in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (0.3.1)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-embeddings-openai) (0.12.19)\n",
      "Requirement already satisfied: openai>=1.1.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-embeddings-openai) (1.58.1)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.0.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.11.12)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2025.3.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.10.6)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.17.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.18.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-embeddings-openai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-embeddings-openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.14.0)\n",
      "Requirement already satisfied: click in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (2.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (3.26.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-openai) (24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://repo.huaweicloud.com/repository/pypi/simple\n",
      "Requirement already satisfied: llama-index-llms-openai in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (0.3.20)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.17 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-llms-openai) (0.12.19)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-llms-openai) (1.58.1)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (2.0.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (3.11.12)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (2025.3.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (2.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (2.10.6)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (1.17.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (1.18.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->llama-index-llms-openai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->llama-index-llms-openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (0.14.0)\n",
      "Requirement already satisfied: click in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (2.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (3.26.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.17->llama-index-llms-openai) (24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://repo.huaweicloud.com/repository/pypi/simple\n",
      "Requirement already satisfied: llama-index-readers-file in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (0.4.5)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-readers-file) (4.13.3)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-readers-file) (0.12.19)\n",
      "Requirement already satisfied: pandas in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-readers-file) (2.2.3)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-readers-file) (5.4.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-readers-file) (0.0.26)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (2.0.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (3.11.12)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (2025.3.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (2.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (2.10.6)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (4.67.1)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.17.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from pandas->llama-index-readers-file) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from pandas->llama-index-readers-file) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from pandas->llama-index-readers-file) (2025.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.18.3)\n",
      "Requirement already satisfied: click in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (3.26.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (0.14.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (24.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-file) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://repo.huaweicloud.com/repository/pypi/simpleNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: llama-index-vector-stores-faiss in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (0.3.0)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-vector-stores-faiss) (0.12.19)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2.0.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (3.11.12)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2025.3.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2.10.6)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.18.3)\n",
      "Requirement already satisfied: click in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (3.26.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (0.14.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (24.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-faiss) (1.3.1)\n",
      "Looking in indexes: https://repo.huaweicloud.com/repository/pypi/simpleNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: llamaindex-py-client in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (0.1.19)\n",
      "Requirement already satisfied: httpx>=0.20.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llamaindex-py-client) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=1.10 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llamaindex-py-client) (2.10.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from httpx>=0.20.0->llamaindex-py-client) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from httpx>=0.20.0->llamaindex-py-client) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from httpx>=0.20.0->llamaindex-py-client) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from httpx>=0.20.0->llamaindex-py-client) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from httpcore==1.*->httpx>=0.20.0->llamaindex-py-client) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from pydantic>=1.10->llamaindex-py-client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from pydantic>=1.10->llamaindex-py-client) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from pydantic>=1.10->llamaindex-py-client) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from anyio->httpx>=0.20.0->llamaindex-py-client) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from anyio->httpx>=0.20.0->llamaindex-py-client) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-core\n",
    "%pip install llama-index-embeddings-openai\n",
    "%pip install llama-index-llms-openai\n",
    "%pip install llama-index-readers-file\n",
    "%pip install llama-index-vector-stores-faiss\n",
    "%pip install llamaindex-py-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们想要借助llama_index的OpenAI接口使用其他厂家的模型，通过翻阅源码，发现llama index 把OpenAI和OpenAIEmbedding的模型名称写死在代码里了，它会检查每个模型的输入上下文大小，如果模型没有在他的列表中，就会报错。所以我们可以重写一下llama_index的OpenAI类，通过新建一个NewOpenAI类，并继承OpenAI类，我们直接把输入上下文大小写死，不让它检查了，它就不报错了。\n",
    "\n",
    "重写OpenAI类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.base.llms.types import LLMMetadata,MessageRole\n",
    "class NewOpenAI(OpenAI):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        # 创建一个新的LLMMetadata实例，只修改context_window\n",
    "        return LLMMetadata(\n",
    "            context_window=8192,\n",
    "            num_output=self.max_tokens or -1,\n",
    "            is_chat_model=True,\n",
    "            is_function_calling_model=True,\n",
    "            model_name=self.model,\n",
    "            system_role=MessageRole.USER,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重写完后，我们用NewOpenAI这个类来配置llm。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = NewOpenAI(\n",
    "    temperature=0.95,\n",
    "    api_key = api_key,\n",
    "    model = chat_model,\n",
    "    api_base = base_url # 注意这里单词不一样\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试对话模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"你是谁？\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "我是一个\n",
      "\n",
      "我是一个人工智能\n",
      "\n",
      "我是一个人工智能助手\n",
      "\n",
      "我是一个人工智能助手，\n",
      "\n",
      "我是一个人工智能助手，名为\n",
      "\n",
      "我是一个人工智能助手，名为 Chat\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGL\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 K\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司于\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司于 \n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司于 202\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户的问题\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。\n",
      "\n",
      "我是一个人工智能助手，名为 ChatGLM，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。"
     ]
    }
   ],
   "source": [
    "# 测试对话模型流式输出\n",
    "response = llm.stream_complete(\"你是谁?\")\n",
    "for chunk in response:\n",
    "    print('\\n')\n",
    "    print(chunk, end=\"\",flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。\n",
    "\n",
    "除了重写类，我们也可以直接去源代码文件去修改源代码。\n",
    "\n",
    "### 修改对话模型源码\n",
    "找到这个文件：Lib\\site-packages\\llama_index\\llms\\openai\\utils.py\n",
    "\n",
    "在大约第30行，有一个GPT4_MODELS: Dict[str, int]\n",
    "在这里面列举了很多模型的名称，把智谱的glm-4-flash加进入，变成这样：\n",
    "\n",
    "\"gpt-4\": 8192,\n",
    "\"gpt-4-32k\": 32768,\n",
    "\n",
    "改成：\n",
    "\n",
    "\"gpt-4\": 8192,\n",
    "\"glm-4-flash\": 8192,\n",
    "\"gpt-4-32k\": 32768,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 修改嵌入模型源码\n",
    "\n",
    "改源码：\n",
    "Lib\\site-packages\\llama_index\\embeddings\\openai\\base.py\n",
    "\n",
    "一共改四个地方\n",
    "\n",
    "class OpenAIEmbeddingModelType(str, Enum):\n",
    "最下面增加 \n",
    "EMBED_2 = \"embedding-2\"\n",
    "\n",
    "class OpenAIEmbeddingModeModel(str, Enum):\n",
    "最下面增加 \n",
    "EMBED_2 = \"embedding-2\"\n",
    "\n",
    "_QUERY_MODE_MODEL_DICT = {\n",
    "最下面增加 \n",
    "(OAEM.TEXT_SEARCH_MODE, \"embedding-2\"): OAEMM.EMBED_2,\n",
    "    \n",
    "_TEXT_MODE_MODEL_DICT = {\n",
    "最下面增加 \n",
    "(OAEM.TEXT_SEARCH_MODE, \"embedding-2\"): OAEMM.EMBED_2,\n",
    "\n",
    "改了上面这四个地方，再调用OpenAIEmbedding这个类，就可以正常使用了\n",
    "如果怕麻烦，也可以用教程里附的base.py 直接替换。\n",
    "\n",
    "如果不小心导入过llama_index，改完源码不要忘了重启jupyter内核。\n",
    "\n",
    "### 运行对话模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。\n",
      "\n",
      "\n",
      "我是一个\n",
      "\n",
      "我是一个名为\n",
      "\n",
      "我是一个名为 Chat\n",
      "\n",
      "我是一个名为 ChatGL\n",
      "\n",
      "我是一个名为 ChatGLM\n",
      "\n",
      "我是一个名为 ChatGLM \n",
      "\n",
      "我是一个名为 ChatGLM 的人工\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 K\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 \n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 202\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户的问题\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。\n",
      "\n",
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(\n",
    "    temperature=0.95,\n",
    "    api_key = api_key,\n",
    "    model = chat_model,\n",
    "    api_base = base_url # 注意这里单词不一样\n",
    ")\n",
    "\n",
    "# 测试对话模型\n",
    "response = llm.complete(\"你是谁？\")\n",
    "print(response)\n",
    "\n",
    "# 测试对话模型流失输出\n",
    "response = llm.stream_complete(\"你是谁？\")\n",
    "for chunk in response:\n",
    "    print('\\n')\n",
    "    print(chunk, end=\"\",flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 运行嵌入模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, list)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 配置Embedding模型\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "embedding = OpenAIEmbedding(\n",
    "    api_key = api_key,\n",
    "    model = emb_model,\n",
    "    api_base = base_url # 注意这里单词不一样\n",
    ")\n",
    "\n",
    "emb = embedding.get_text_embedding(\"你好呀呀\")\n",
    "len(emb), type(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出 (1024, list)\n",
    "\n",
    "说明配置成功。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三个思路：自定义模型接口\n",
    "\n",
    "自定义可以利用openai-like的包，来封装任何openai类似的大模型\n",
    "这个思路的缺点很明显，只有对话模型，没有嵌入模型。\n",
    "\n",
    "虽然不推荐这个方式，但是如果实在想要尝试，先安装这个库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://repo.huaweicloud.com/repository/pypi/simple\n",
      "Requirement already satisfied: llama-index-llms-openai-like in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (0.3.4)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-llms-openai-like) (0.12.19)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.9 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-llms-openai-like) (0.3.20)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.37.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-llms-openai-like) (4.49.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (2.0.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (3.11.12)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (2025.3.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (2.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (2.10.6)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (1.17.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from llama-index-llms-openai<0.4.0,>=0.3.9->llama-index-llms-openai-like) (1.58.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like) (0.29.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like) (24.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like) (0.5.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (1.18.3)\n",
      "Requirement already satisfied: click in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (1.4.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai<0.4.0,>=0.3.9->llama-index-llms-openai-like) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai<0.4.0,>=0.3.9->llama-index-llms-openai-like) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai<0.4.0,>=0.3.9->llama-index-llms-openai-like) (0.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai<0.4.0,>=0.3.9->llama-index-llms-openai-like) (1.3.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (2.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\huqi\\.conda\\envs\\wow-rag\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-openai-like) (3.26.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\huqi\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->llama-index-llms-openai<0.4.0,>=0.3.9->llama-index-llms-openai-like) (1.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-llms-openai-like\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对话模型可以直接使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai_like import OpenAILike\n",
    "\n",
    "llm = OpenAILike(\n",
    "    model = chat_model,\n",
    "    api_base = base_url,\n",
    "    api_key = api_key,\n",
    "    is_chat_model=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义对话模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库和模块\n",
    "from openai import OpenAI\n",
    "from pydantic import Field  # 导入Field，用于Pydantic模型中定义字段的元数据\n",
    "from typing import Optional, List, Mapping, Any, Generator\n",
    "import os\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, SummaryIndex\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "from llama_index.core.llms import (\n",
    "    CustomLLM,\n",
    "    CompletionResponse,\n",
    "    CompletionResponseGen,\n",
    "    LLMMetadata,\n",
    ")\n",
    "from llama_index.core.llms.callbacks import llm_completion_callback\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# 定义OurLLM类，继承自CustomLLM基类\n",
    "class OurLLM(CustomLLM):\n",
    "    api_key: str = Field(default=api_key)\n",
    "    base_url: str = Field(default=base_url)\n",
    "    model_name: str = Field(default=chat_model)\n",
    "    client: OpenAI = Field(default=None, exclude=True)  # 显式声明 client 字段\n",
    "\n",
    "    def __init__(self, api_key: str, base_url: str, model_name: str = chat_model, **data: Any):\n",
    "        super().__init__(**data)\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.model_name = model_name\n",
    "        self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)  # 使用传入的api_key和base_url初始化 client 实例\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"Get LLM metadata.\"\"\"\n",
    "        return LLMMetadata(\n",
    "            model_name=self.model_name,\n",
    "        )\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
    "        response = self.client.chat.completions.create(model=self.model_name, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        if hasattr(response, 'choices') and len(response.choices) > 0:\n",
    "            response_text = response.choices[0].message.content\n",
    "            return CompletionResponse(text=response_text)\n",
    "        else:\n",
    "            raise Exception(f\"Unexpected response format: {response}\")\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(\n",
    "        self, prompt: str, **kwargs: Any\n",
    "    ) -> Generator[CompletionResponse, None, None]:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            for chunk in response:\n",
    "                chunk_message = chunk.choices[0].delta\n",
    "                if not chunk_message.content:\n",
    "                    continue\n",
    "                content = chunk_message.content\n",
    "                yield CompletionResponse(text=content, delta=content)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Unexpected response format: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。\n",
      "\n",
      "\n",
      "我是一个\n",
      "\n",
      "名为\n",
      "\n",
      " Chat\n",
      "\n",
      "GL\n",
      "\n",
      "M\n",
      "\n",
      " \n",
      "\n",
      "的人工\n",
      "\n",
      "智能\n",
      "\n",
      "助手\n",
      "\n",
      "，\n",
      "\n",
      "是基于\n",
      "\n",
      "清华大学\n",
      "\n",
      " K\n",
      "\n",
      "EG\n",
      "\n",
      " 实\n",
      "\n",
      "验\n",
      "\n",
      "室\n",
      "\n",
      "和\n",
      "\n",
      "智\n",
      "\n",
      "谱\n",
      "\n",
      " AI\n",
      "\n",
      " 公司\n",
      "\n",
      "于\n",
      "\n",
      " \n",
      "\n",
      "202\n",
      "\n",
      "4\n",
      "\n",
      " 年\n",
      "\n",
      "共同\n",
      "\n",
      "训练\n",
      "\n",
      "的语言\n",
      "\n",
      "模型\n",
      "\n",
      "开发的\n",
      "\n",
      "。\n",
      "\n",
      "我的\n",
      "\n",
      "任务是\n",
      "\n",
      "针对\n",
      "\n",
      "用户\n",
      "\n",
      "的问题\n",
      "\n",
      "和要求\n",
      "\n",
      "提供\n",
      "\n",
      "适当的\n",
      "\n",
      "答复\n",
      "\n",
      "和支持\n",
      "\n",
      "。"
     ]
    }
   ],
   "source": [
    "# 测试对话模型\n",
    "llm = OurLLM(api_key=api_key, base_url=base_url, model_name=chat_model)\n",
    "response = llm.complete(\"你是谁？\")\n",
    "print(response)\n",
    "\n",
    "# 测试对话模型流失输出\n",
    "response = llm.stream_complete(\"你是谁？\")\n",
    "for chunk in response:\n",
    "    print('\\n')\n",
    "    print(chunk, end=\"\",flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我是一个名为 ChatGLM 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2024 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。\n",
    "\n",
    "### 自定义嵌入模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from typing import Any, List\n",
    "from llama_index.core.embeddings import BaseEmbedding\n",
    "from pydantic import Field\n",
    "\n",
    "\n",
    "class OurEmbeddings(BaseEmbedding):\n",
    "    api_key: str = Field(default=api_key)\n",
    "    base_url: str = Field(default=base_url)\n",
    "    model_name: str = Field(default=emb_model)\n",
    "    client: OpenAI = Field(default=None, exclude=True)  # 显式声明 client 字段\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: str = api_key, \n",
    "        base_url: str = base_url,\n",
    "        model_name: str = emb_model,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.model_name = model_name\n",
    "        self.client = OpenAI(api_key=self.api_key, base_url=self.base_url) \n",
    "\n",
    "    def invoke_embedding(self, query: str) -> List[float]:\n",
    "        response = self.client.embeddings.create(model=self.model_name, input=[query])\n",
    "\n",
    "        # 检查响应是否成功\n",
    "        if response.data and len(response.data) > 0:\n",
    "            return response.data[0].embedding\n",
    "        else:\n",
    "            raise ValueError(\"Failed to get embedding from ZhipuAI API\")\n",
    "\n",
    "    def _get_query_embedding(self, query: str) -> List[float]:\n",
    "        return self.invoke_embedding(query)\n",
    "\n",
    "    def _get_text_embedding(self, text: str) -> List[float]:\n",
    "        return self.invoke_embedding(text)\n",
    "\n",
    "    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [self._get_text_embedding(text) for text in texts]\n",
    "\n",
    "    async def _aget_query_embedding(self, query: str) -> List[float]:\n",
    "        return self._get_query_embedding(query)\n",
    "\n",
    "    async def _aget_text_embedding(self, text: str) -> List[float]:\n",
    "        return self._get_text_embedding(text)\n",
    "\n",
    "    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self._get_text_embeddings(texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "来测试一下自定义的嵌入模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, list)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = OurEmbeddings(api_key=api_key, base_url=base_url, model_name=emb_model)\n",
    "emb = embedding.get_text_embedding(\"你好呀呀\")\n",
    "len(emb), type(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1024, list)\n",
    "\n",
    "# 第四个思路：我们可以在本地安装Ollama\n",
    "Ollama里的qwen2系列非常棒，有很多尺寸规模的模型，最小的模型只有 0.5b,然后还有1.5b、7b、32b等等，可以适应各种设备。\n",
    "访问 https://ollama.com。 下载Windows版本。直接安装。 安装完成后，打开命令行窗口，输入 ollama，如果出现 Usage: Available Commands: 之类的信息，说明安装成功。\n",
    "我们用qwen2:7b这个模型就行，整个还不到4G。 运行 ollama run qwen2:7b\n",
    "如果出现了success，就说明安装成功。 然后会出现一个>>>符号，这就是对话窗口。可以直接输入问题。\n",
    "想要退出交互页面，直接输入 /bye 就行。斜杠是需要的。否则不是退出交互页面，而是对大模型说话，它会继续跟你聊。\n",
    "在浏览器中输入 127.0.0.1:11434，如果出现 Ollama is running\n",
    "说明端口运行正常。\n",
    "\n",
    "安装完ollama后，我们还需要进行配置一下，主要是两个方面。这个配置只有在Windows电脑上试过。Linux和Mac抱歉还没有试过，需要的朋友可能要自己探索一下了。\n",
    "第一：这时候模型是放在内存中的。我们希望把模型放在硬盘中。所以，我们可以在硬盘中建一个文件夹，比如： D:\\programs\\ollama\\models\n",
    "然后新建系统环境变量。 变量名： OLLAMA_MODELS\n",
    "变量值： D:\\programs\\ollama\\models\n",
    "第二：这时候的大模型只能通过127.0.0.1:11434来访问。我们希望在局域网中的任何电脑都可以访问。这也是通过新建环境变量来解决。\n",
    "变量名： OLLAMA_HOST 变量值： 0.0.0.0:11434\n",
    "这样就完成了配置。是不是非常简单方便？\n",
    "\n",
    "如果是在jupyter notebook里运行，需要先用魔法命令安装这些库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-core\n",
    "%pip install llama-index-embeddings-ollama\n",
    "%pip install llama-index-llms-ollama\n",
    "%pip install llama-index-readers-file\n",
    "%pip install llama-index-vector-stores-faiss\n",
    "%pip install llamaindex-py-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们先用requets库来测试一下大模型\n",
    "import json\n",
    "import requests\n",
    "# 192.168.0.123就是部署了大模型的电脑的IP，\n",
    "# 请根据实际情况进行替换\n",
    "BASE_URL = \"http://192.168.0.123:11434/api/chat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接输出看看：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "  \"model\": \"qwen2:7b\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"请写一篇1000字左右的文章，论述法学专业的就业前景。\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "response = requests.post(BASE_URL, json=payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "然后改成流式输出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "  \"model\": \"qwen2:7b\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"请写一篇1000字左右的文章，论述法学专业的就业前景。\"\n",
    "    }\n",
    "  ],\n",
    "  \"stream\": True\n",
    "}\n",
    "response = requests.post(BASE_URL, json=payload, stream=True)  # 在这里设置stream=True告诉requests不要立即下载响应内容  \n",
    "# 检查响应状态码  \n",
    "if response.status_code == 200:  \n",
    "    # 使用iter_content()迭代响应体  \n",
    "    for chunk in response.iter_content(chunk_size=1024):  # 你可以设置chunk_size为你想要的大小  \n",
    "        if chunk:  \n",
    "            # 在这里处理chunk（例如，打印、写入文件等）  \n",
    "            rtn = json.loads(chunk.decode('utf-8')) # 假设响应是文本，并且使用UTF-8编码  \n",
    "            print(rtn[\"message\"][\"content\"], end=\"\")\n",
    "else:  \n",
    "    print(f\"Error: {response.status_code}\")  \n",
    "\n",
    "# 不要忘记关闭响应  \n",
    "response.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果上面输出正常，说明Ollama的配置成功。我们再来配置一下Llama-index。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置chat模型\n",
    "from llama_index.llms.ollama import Ollama\n",
    "llm = Ollama(base_url=\"http://192.168.0.123:11434\", model=\"qwen2:7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试chat模型\n",
    "response = llm.complete(\"你是谁？\")\n",
    "print(response)\n",
    "# 我是阿里云开发的一款超大规模语言模型，我叫通义千问。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置嵌入模型\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "embedding = OllamaEmbedding(base_url=\"http://192.168.0.123:11434\", model_name=\"qwen2:7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试嵌入模型\n",
    "emb = embedding.get_text_embedding(\"你好呀呀\")\n",
    "len(emb), type(emb)\n",
    "# (3584, list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是有个问题，就是Ollama这个嵌入模型用来做向量检索，效果不好。一般来说向量的相似度得分是在0到1之间的小数。但是Ollama这个嵌入模型计算出来的得分要么非常大，要么非常小。根据经验，还是智谱的这个向量模型效果最好。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wow-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
